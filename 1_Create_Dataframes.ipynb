{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL IMPORTS\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(countries, athletes, events):\n",
    "    \"\"\" This functions takes the 3 .csv files --> \n",
    "    Convert them into a panda dataframe -->\n",
    "    Do some datacleaning --> \n",
    "    add the WikiLink column\n",
    "    \n",
    "    Input: 3x .csv files\n",
    "    -------------------------------\n",
    "    Output: 3x dataframes\n",
    "    \"\"\"\n",
    "    df_countries = pd.read_csv(countries, sep = ';',   encoding = 'latin-1', dtype = object) # Convert countries.csv into panda dataframe\n",
    "    df_countries['WikiLink'] = df_countries['country']+\"_at_the_2016_Summer_Olympics\"        # Add a column for WikiLinks\n",
    "    df_countries['country2'] = df_countries['country']                                       # Add a column country2 (Will be used for finding athletes WikiLink)\n",
    "    df_countries['country2'] = df_countries['country2'].str.replace('\\s', '_')               # Replase space with _\n",
    "    df_countries['WikiLink'] = df_countries['WikiLink'].str.replace('\\s', '_')               # Replase space with _\n",
    "    df_countries['WikiLink'] = df_countries['WikiLink'].str.replace(\"*\", \"\")                 # Remove *\n",
    "    df_countries = df_countries.dropna()                                                     # Remove NanN\n",
    "    \n",
    "    df_athletes = pd.read_csv(athletes, sep = ',',   encoding = 'utf-8', dtype = object)     # Convert athletes.csv to panda dataframe\n",
    "    df_athletes['name2'] = df_athletes['name']                                               # Add a column name2 (Will be used for finding athletes WikiLink)\n",
    "    df_athletes['name2'] = df_athletes['name2'].str.replace('\\s', '_')                       # Replase space with _\n",
    "    df_athletes = df_athletes.dropna()                                                       # Remove NanN\n",
    "    \n",
    "    df_events = pd.read_csv(events, sep = ';', encoding = 'latin-1', dtype = object)         # Convert events.csv to panda dataframe\n",
    "    df_events['WikiLink'] = df_events['sport']+\"_at_the_2016_Summer_Olympics\"                # Add a column for WikiLinks\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('canoe', 'canoeing')           # Replace canou (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('hockey', 'field hockey')      # Replace hockey (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('synchronised','synchronized') # Replace synchronised (for Matching Wikilink)\n",
    "    df_events['WikiLink'] = df_events['WikiLink'].str.replace('\\s', '_')                     # Replase space with _\n",
    "    df_events = df_events.drop_duplicates(subset = [\"WikiLink\"])                             # drop duplicates\n",
    "    df_events = df_events.dropna()                                                           # Remove NanN\n",
    "    \n",
    "    return df_countries, df_athletes, df_events\n",
    "\n",
    "df_countries,df_athletes, df_events = read_file('countries.csv','athletes.csv','events.csv')\n",
    "\n",
    "\n",
    "def Athlet_WikiLink(df1, df2):\n",
    "    \"\"\" This functions finds the athletes wikilinks by \n",
    "    find all wikilinks from the coutries wikipages. \n",
    "    If the wikilinks matches a name from the athletes dataframe\n",
    "    save the wikilink in a list.\n",
    "    \n",
    "    Input: athletes dataframe, countries dataframe\n",
    "    -------------------------------\n",
    "    Output: list with athletes Wikilinks\n",
    "    \"\"\"\n",
    "    A_WikiLink = []                                                                          # Create an empty list\n",
    "    path_folder = (\"./Files/\")                                                               # Folder with all the countries json wikipages\n",
    "    for i in df2['country2']:                                                                # Run through all countries\n",
    "        f = io.open(path_folder + 'countries_'+ i + '_at_the_2016_Summer_Olympics.txt','r',encoding = 'utf-8').read() # Open the file\n",
    "        links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)                                               # Find all links\n",
    "        links = [x.replace(' ','_') for x in links]                                          # Replase space with _\n",
    "        links = [s.split('|') for s in links]                                                # Split the links by the '|'\n",
    "\n",
    "        for j in range(len(links)):                                                          # Run through all links\n",
    "                if len(links[j]) == 1:                                                       # If the name and the links are the same\n",
    "                    if len(df1.loc[df1['name2'] == links[j][0]]) >= 1:                       # If the link excist in the athletes dataframe\n",
    "                        A_WikiLink.append([df1.at[df1.loc[df1['name2'] == links[j][0]].index[0],'id'],links[j][0]]) # append the wikilink and the 'id' number to the list\n",
    "                elif len(links[j]) == 2:                                                     # If the name and the links are not the same\n",
    "                    if len(df1.loc[df1['name2'] == links[j][1]]) >= 1:                       # If the link excist in the athletes dataframe\n",
    "                        A_WikiLink.append([df1.at[df1.loc[df1['name2'] == links[j][1]].index[0],'id'],links[j][0]]) # append the wikilink and the 'id' number to the list\n",
    "    return A_WikiLink\n",
    "\n",
    "A_WikiLink = Athlet_WikiLink(df_athletes,df_countries)           # Run Function\n",
    "\n",
    "df_if = pd.DataFrame(A_WikiLink, columns =['id', 'WikiLink'])    # Convert wikilink list into panda dataframe\n",
    "df_athletes = df_athletes.merge(df_if)                           # Merge wikilink dataframe and athletes dataframe\n",
    "df_athletes = df_athletes.drop_duplicates()                      # Drop duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes:\n",
    "pickle.dump(df_athletes, open('df_athletes.txt', 'wb'))\n",
    "pickle.dump(df_countries, open('df_countries.txt', 'wb'))\n",
    "pickle.dump(df_events, open('df_events.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

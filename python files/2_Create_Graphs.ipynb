{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL IMPORTS\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes\n",
    "df_athletes = pickle.load(open('df_athletes.txt','rb'))\n",
    "df_countries = pickle.load(open('df_countries.txt','rb'))\n",
    "df_events = pickle.load(open('df_events.txt','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def findLinksAndRemove(f,df1,df2,df3):\n",
    "    \"\"\" Helperfunction to find links in wikipages \n",
    "    and remove the link if it is not a wikilink \n",
    "    in the dataframes.\n",
    "    \n",
    "    Input: the file, and the 3 dataframes\n",
    "    -------------------------------\n",
    "    Output: list of edges\n",
    "    \"\"\"\n",
    "    links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)                         # Use a regular expression to extract all outgoing links\n",
    "    links = [x.replace(' ','_') for x in links]                    # Replace space with _\n",
    "    links = [s.split('|') for s in links]                          # Split the links by the '|'\n",
    "    \n",
    "    \n",
    "    edges = []                                                     # An empty list for edges\n",
    "    for i in range(len(links)):                                    # Run through all links\n",
    "                                                                   # For each link, check if the target is in the data.\n",
    "        if len(df1.loc[df1['WikiLink'] == links[i][0]]) >= 1 or len(df2.loc[df2['WikiLink'] == links[i][0]]) >= 1 or len(df3.loc[df3['WikiLink'] == links[i][0]]) >= 1:\n",
    "            edges.append(links[i][0])                              # If yes add the link to the edge list. If no, discard it.\n",
    "    return(edges)\n",
    "\n",
    "\n",
    "def AddNodes(G, df, nodetype):\n",
    "    \"\"\" Function to add nodes to the graph. \n",
    "    Every wikilink in the data is a node in the graph.\n",
    "     \n",
    "    Input: The NetworkX DiGraph, The dataframe and the nodetype\n",
    "    -------------------------------\n",
    "    Output: The NetworkX DiGraph\n",
    "    \"\"\"\n",
    "    for i in range(df.shape[0]):                                  # Run through all wikilinks in the dataframe\n",
    "        G.add_node(df.WikiLink.iloc[i], nodetype = nodetype)      # Add the node to the Graph\n",
    " \n",
    "   \n",
    "def AddEdges(G, df1, df2, df3, nodetype):\n",
    "    \"\"\" Function to add edges to the graph. \n",
    "     \n",
    "    Input: The NetworkX DiGraph, all dataframe and the nodetype\n",
    "    -------------------------------\n",
    "    Output: The NetworkX DiGraph\n",
    "    \"\"\"\n",
    "    path_folder = (\"./Files/\")                                    # Folder with all the downloaded wikipages\n",
    "    for i in range(df1.shape[0]):                                 # Run through all wikilinks in the dataframe                                                                  \n",
    "        Node = df1['WikiLink'].iloc[i]                            # Open the page file\n",
    "        f = io.open(path_folder + nodetype + Node + \".txt\",'r',encoding = 'utf-8').read()\n",
    "        edgesTo = findLinksAndRemove(f,df1,df2,df3)               # Run the helperfuncktion to find links and remove the link if it is not in the data.\n",
    "        \n",
    "        for j in edgesTo:                                         # Run through all the finded edges\n",
    "            if j in list(G.nodes):                                # If the edge link to a node add edge to Graph\n",
    "                G.add_edge(Node, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a NetworkX DiGraph to store the network. Store also the properties of the nodes (i.e. from which dataframe they hail).\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add Nodes\n",
    "AddNodes(G, df_countries, 'countries')\n",
    "AddNodes(G, df_events, 'events')\n",
    "AddNodes(G, df_athletes, 'athletes')\n",
    "\n",
    "# Add edges\n",
    "AddEdges(G, df_countries, df_events, df_athletes, 'countries_')\n",
    "AddEdges(G, df_events, df_countries, df_athletes, 'events_')\n",
    "AddEdges(G, df_athletes, df_events, df_countries, 'athletes_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if nodes do not have any out- or in- degrees. These may discard from the network.\n",
    "remove = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "G.remove_nodes_from(remove) \n",
    "\n",
    "# Save the graph.\n",
    "pickle.dump(G, open('G.txt', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
